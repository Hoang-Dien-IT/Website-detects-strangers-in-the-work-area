import cv2
import asyncio
import numpy as np
from typing import Dict, Any, Optional, AsyncGenerator
from ..models.camera import CameraResponse
from ..services.face_processor import face_processor
from ..services.websocket_manager import websocket_manager
from ..services.detection_tracker import detection_tracker
from ..services.detection_optimizer_service import DetectionOptimizerService
import concurrent.futures
import time
import base64
from io import BytesIO
from PIL import Image, ImageDraw, ImageFont
import os
import asyncio
import numpy as np
from typing import Dict, Any, Optional, AsyncGenerator, List, List
from ..models.camera import CameraResponse
from ..services.face_processor import face_processor
from ..services.websocket_manager import websocket_manager
import concurrent.futures
import time
import base64
from io import BytesIO

class StreamProcessor:
    def __init__(self):
        self.active_streams: Dict[str, Dict[str, Any]] = {}
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=4)
        self._frame_times: Dict[str, float] = {}  # Äá»ƒ tracking FPS

    async def get_stream_info(self, camera_id: str) -> Dict[str, Any]:
        """Láº¥y thÃ´ng tin stream"""
        if camera_id in self.active_streams:
            stream = self.active_streams[camera_id]
            return {
                "is_streaming": stream.get("is_active", False),
                "status": "online" if stream.get("is_active") else "offline",
                "viewers_count": stream.get("viewers_count", 0),
                "uptime": time.time() - stream.get("start_time", time.time())
            }
        else:
            return {
                "is_streaming": False,
                "status": "offline",
                "viewers_count": 0,
                "uptime": 0
            }

    import threading
    import queue

    def _frame_reader(self, camera_id: str, camera: CameraResponse, frame_queue: 'queue.Queue', stop_event: 'threading.Event'):
        """Luá»“ng Ä‘á»c frame liÃªn tá»¥c cho camera"""
        if camera.camera_type == "webcam":
            cap = cv2.VideoCapture(0)
        elif camera.camera_url:
            cap = cv2.VideoCapture(camera.camera_url)
        else:
            cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            return
        self.active_streams[camera_id]["cap"] = cap
        while not stop_event.is_set():
            ret, frame = cap.read()
            if not ret:
                continue
            # Giá»¯ queue chá»‰ 1 frame má»›i nháº¥t (loáº¡i bá» frame cÅ©)
            if not frame_queue.empty():
                try:
                    frame_queue.get_nowait()
                except Exception:
                    pass
            frame_queue.put(frame)
        cap.release()

    async def start_stream(self, camera_id: str, camera: CameraResponse) -> bool:
        """Báº¯t Ä‘áº§u stream camera (tá»‘i Æ°u Ä‘a luá»“ng Ä‘á»c frame)"""
        try:
            if camera_id in self.active_streams:
                return True  # Already streaming
            detection_tracker.start_cleanup_task()
            import threading
            import queue
            frame_queue = queue.Queue(maxsize=1)
            stop_event = threading.Event()
            reader_thread = threading.Thread(target=self._frame_reader, args=(camera_id, camera, frame_queue, stop_event), daemon=True)
            self.active_streams[camera_id] = {
                "camera": camera,
                "is_active": True,
                "start_time": time.time(),
                "viewers_count": 0,
                "cap": None,
                "frame_queue": frame_queue,
                "stop_event": stop_event,
                "reader_thread": reader_thread
            }
            reader_thread.start()
            print(f"Stream started for camera: {camera.name}")
            return True
        except Exception as e:
            print(f"Error starting stream: {e}")
            return False

    async def stop_stream(self, camera_id: str) -> bool:
        """Dá»«ng stream camera (tá»‘i Æ°u Ä‘a luá»“ng)"""
        try:
            if camera_id in self.active_streams:
                stream = self.active_streams[camera_id]
                stream["is_active"] = False
                # Stop frame reader thread
                if stream.get("stop_event"):
                    stream["stop_event"].set()
                if stream.get("reader_thread"):
                    stream["reader_thread"].join(timeout=1)
                # Close camera capture if exists
                if stream.get("cap"):
                    stream["cap"].release()
                del self.active_streams[camera_id]
                if not self.active_streams:
                    await detection_tracker.stop_cleanup_task()
                print(f"Stream stopped for camera: {camera_id}")
                return True
            return False
        except Exception as e:
            print(f"Error stopping stream: {e}")
            return False

    async def generate_video_stream(self, camera_id: str, camera: CameraResponse) -> AsyncGenerator[bytes, None]:
        """Generate video stream frames (tá»‘i Æ°u Ä‘a luá»“ng, AI má»—i 5 frame)"""
        try:
            await self.start_stream(camera_id, camera)
            stream = self.active_streams.get(camera_id)
            if not stream or "frame_queue" not in stream:
                async for frame in self._generate_dummy_frames():
                    yield frame
                return
            frame_queue = stream["frame_queue"]
            frame_count = 0
            last_ai_result = None
            while True:
                if camera_id not in self.active_streams or not self.active_streams[camera_id]["is_active"]:
                    break
                try:
                    frame = frame_queue.get(timeout=1)
                except Exception:
                    frame = self._create_dummy_frame(f"Camera {camera.name} - No Signal")
                frame_count += 1
                # Chá»‰ xá»­ lÃ½ AI má»—i 5 frame, cÃ¡c frame cÃ²n láº¡i chá»‰ overlay láº¡i káº¿t quáº£ AI cÅ©
                if camera.detection_enabled and frame_count % 5 == 0:
                    processed_frame = await self._process_frame(frame.copy(), camera_id, camera)
                    last_ai_result = processed_frame
                elif last_ai_result is not None:
                    processed_frame = last_ai_result.copy()
                    # Cáº­p nháº­t overlay thá»i gian, FPS, camera name
                    current_time = time.time()
                    fps = 0
                    if hasattr(self, '_frame_times') and camera_id in self._frame_times:
                        fps = 1.0 / (current_time - self._frame_times[camera_id])
                        self._frame_times[camera_id] = current_time
                    cv2.putText(processed_frame, f"FPS: {fps:.2f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                    cv2.putText(processed_frame, f"Camera: {camera.name}", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
                    cv2.putText(processed_frame, timestamp, (10, processed_frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                else:
                    processed_frame = await self._process_frame(frame.copy(), camera_id, camera)
                _, buffer = cv2.imencode('.jpg', processed_frame, [cv2.IMWRITE_JPEG_QUALITY, 70])
                frame_bytes = buffer.tobytes()
                yield (b'--frame\r\n'
                       b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
                await asyncio.sleep(0.001)  # sleep ráº¥t nhá» Ä‘á»ƒ trÃ¡nh block event loop
        except Exception as e:
            print(f"Error in video stream: {e}")
            async for frame in self._generate_error_frames(str(e)):
                yield frame

    async def _process_frame(self, frame: np.ndarray, camera_id: str, camera: CameraResponse) -> np.ndarray:
        """Process frame (add overlays, detection, etc.) - theo code máº«u"""
        try:
            # TÃ­nh FPS giá»‘ng code máº«u
            current_time = time.time()
            if not hasattr(self, '_frame_times'):
                self._frame_times = {}
            
            if camera_id not in self._frame_times:
                self._frame_times[camera_id] = current_time
                fps = 0
            else:
                fps = 1.0 / (current_time - self._frame_times[camera_id])
                self._frame_times[camera_id] = current_time
            
            # Hiá»ƒn thá»‹ FPS lÃªn gÃ³c trÃªn bÃªn trÃ¡i giá»‘ng code máº«u
            cv2.putText(frame, f"FPS: {fps:.2f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            
            # Add camera name
            cv2.putText(frame, f"Camera: {camera.name}", (10, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # Add timestamp
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
            cv2.putText(frame, timestamp, (10, frame.shape[0] - 10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            
            # Face detection vÃ  recognition giá»‘ng code máº«u
            if camera.detection_enabled:
                cv2.putText(frame, "DETECTION: ON", (frame.shape[1] - 150, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
                
                try:
                    # Load known persons for recognition
                    known_persons = await self._get_known_persons_for_camera(camera_id)
                    
                    # PhÃ¡t hiá»‡n vÃ  nháº­n dáº¡ng khuÃ´n máº·t vá»›i detection tracking
                    detections = await face_processor.detect_and_recognize_faces(frame, known_persons)
                    
                    # Sá»­ dá»¥ng detection_tracker Ä‘á»ƒ quyáº¿t Ä‘á»‹nh cÃ³ lÆ°u detection hay khÃ´ng
                    for detection in detections:
                        person_name = detection.get('person_name', 'Unknown')
                        person_id = detection.get('person_id')
                        confidence = detection.get('confidence', 0)
                        
                        # XÃ¡c Ä‘á»‹nh loáº¡i detection
                        detection_type = "known_person" if person_name != "Unknown" else "stranger"
                        
                        # Sá»­ dá»¥ng detection_tracker Ä‘á»ƒ quyáº¿t Ä‘á»‹nh cÃ³ lÆ°u hay khÃ´ng
                        should_save = detection_tracker.track_detection(
                            camera_id=camera_id,
                            person_id=person_id or f"unknown_{int(time.time())}",
                            person_name=person_name,
                            detection_type=detection_type,
                            confidence=confidence
                        )
                        
                        # ÄÃ¡nh dáº¥u náº¿u cáº§n lÆ°u detection nÃ y
                        detection['should_save'] = should_save
                        detection['detection_type'] = detection_type
                
                    # Váº½ cÃ¡c khuÃ´n máº·t Ä‘Ã£ phÃ¡t hiá»‡n vÃ  nháº­n dáº¡ng giá»‘ng code máº«u
                    for detection in detections:
                        # Láº¥y bounding box
                        x, y, w, h = detection.get('bbox', [0, 0, 0, 0])
                        x1, y1, x2, y2 = x, y, x + w, y + h
                        
                        # Nháº­n dáº¡ng khuÃ´n máº·t
                        name = detection.get('person_name', 'Unknown')
                        confidence = detection.get('confidence', 0)
                        
                        # MÃ u khÃ¡c nhau cho ngÆ°á»i Ä‘Ã£ biáº¿t vÃ  khÃ´ng xÃ¡c Ä‘á»‹nh giá»‘ng code máº«u
                        if name == "Unknown":
                            color = (0, 0, 255)  # Äá» cho ngÆ°á»i láº¡
                        else:
                            color = (0, 255, 0)  # Xanh lÃ¡ cho ngÆ°á»i Ä‘Ã£ biáº¿t
                        
                        # Váº½ bounding box
                        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                        
                        # ThÃªm tÃªn vÃ  Ä‘á»™ tin cáº­y giá»‘ng code máº«u - sá»­ dá»¥ng UTF-8 text
                        label = f"{name} ({confidence:.2f})"
                        
                        # Sá»­ dá»¥ng function UTF-8 Ä‘á»ƒ váº½ tiáº¿ng Viá»‡t Ä‘Ãºng
                        frame = self._draw_utf8_text(frame, label, (x1, y1 - 30), 
                                                   font_scale=0.8, color=color, thickness=2)
                        
                        # Chá»‰ lÆ°u vÃ  gá»­i alert náº¿u detection_tracker cho phÃ©p
                        if detection.get('should_save'):
                            # Get region of interest (face crop)
                            x, y, w, h = detection.get('bbox', [0, 0, 0, 0])
                            face_frame = frame.copy()
                            await self._send_detection_alert(camera_id, detection, face_frame)
                    
                    # Add detection count overlay
                    detection_count = len(detections)
                    cv2.putText(frame, f"Faces: {detection_count}", (frame.shape[1] - 150, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)
                    
                except Exception as detection_error:
                    print(f"Face detection error: {detection_error}")
                    cv2.putText(frame, "DETECTION: ERROR", (frame.shape[1] - 150, 30), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
            else:
                cv2.putText(frame, "DETECTION: OFF", (frame.shape[1] - 150, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
            
            return frame
            
        except Exception as e:
            print(f"Error processing frame: {e}")
            return frame

    def _create_dummy_frame(self, message: str = "No Camera") -> np.ndarray:
        """Create dummy frame when camera is not available"""
        frame = np.zeros((480, 640, 3), dtype=np.uint8)
        cv2.putText(frame, message, (50, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        cv2.putText(frame, time.strftime("%Y-%m-%d %H:%M:%S"), (50, 280), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)
        return frame

    async def _generate_dummy_frames(self) -> AsyncGenerator[bytes, None]:
        """Generate dummy frames for testing"""
        while True:
            frame = self._create_dummy_frame("Demo Camera - No Real Camera Connected")
            _, buffer = cv2.imencode('.jpg', frame)
            frame_bytes = buffer.tobytes()
            
            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
            
            await asyncio.sleep(1/30)  # 30 FPS

    async def _generate_error_frames(self, error_message: str) -> AsyncGenerator[bytes, None]:
        """Generate error frames"""
        for _ in range(10):  # Show error for a few frames
            frame = self._create_dummy_frame(f"Error: {error_message}")
            _, buffer = cv2.imencode('.jpg', frame)
            frame_bytes = buffer.tobytes()
            
            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
            
            await asyncio.sleep(1/5)  # 5 FPS for error

    async def capture_snapshot(self, camera_id: str, camera: CameraResponse) -> Optional[bytes]:
        """Chá»¥p áº£nh snapshot tá»« camera"""
        try:
            if camera.camera_type == "webcam":
                cap = cv2.VideoCapture(0)
            elif camera.camera_url:
                cap = cv2.VideoCapture(camera.camera_url)
            else:
                cap = cv2.VideoCapture(0)
            
            if not cap.isOpened():
                # Return dummy image
                frame = self._create_dummy_frame(f"Snapshot - {camera.name}")
                _, buffer = cv2.imencode('.jpg', frame)
                return buffer.tobytes()
            
            ret, frame = cap.read()
            cap.release()
            
            if ret:
                # Process frame
                frame = await self._process_frame(frame, camera_id, camera)
                _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
                return buffer.tobytes()
            else:
                return None
                
        except Exception as e:
            print(f"Error capturing snapshot: {e}")
            return None

    async def get_stream_status(self, camera_id: str) -> Dict[str, Any]:
        """Láº¥y tráº¡ng thÃ¡i chi tiáº¿t cá»§a stream"""
        if camera_id in self.active_streams:
            stream = self.active_streams[camera_id]
            return {
                "is_streaming": stream.get("is_active", False),
                "is_recording": False,  # TODO: Implement recording
                "viewers_count": stream.get("viewers_count", 0),
                "uptime": time.time() - stream.get("start_time", time.time()),
                "frame_rate": 30,  # TODO: Calculate actual FPS
                "resolution": "640x480"  # TODO: Get actual resolution
            }
        else:
            return {
                "is_streaming": False,
                "is_recording": False,
                "viewers_count": 0,
                "uptime": 0,
                "frame_rate": 0,
                "resolution": "Unknown"
            }

    async def _send_detection_alert(self, camera_id: str, detection: Dict[str, Any], frame: np.ndarray = None):
        """Send detection alert via WebSocket and save to database"""
        try:
            # Get camera name
            from ..database import get_database
            from bson import ObjectId
            
            db = get_database()
            camera_data = await db.cameras.find_one({"_id": ObjectId(camera_id)})
            camera_name = camera_data.get("name", "Unknown Camera") if camera_data else "Unknown Camera"
            
            # Save to database using both methods for compatibility
            detection_id = None
            if frame is not None:
                # Use both detection optimizer and normal save for compatibility
                detection_id = await self._save_optimized_detection(camera_id, camera_name, detection, frame)
                if not detection_id:
                    # Fallback to traditional method if optimizer fails
                    detection_id = await self._save_detection_to_database(camera_id, camera_name, detection, frame)
            
            # Create WebSocket message
            alert_message = {
                "type": "detection_alert",
                "data": {
                    "id": detection_id,  # New ID from database
                    "camera_id": camera_id,
                    "camera_name": camera_name,
                    "detection_type": detection.get('detection_type', 'unknown'),
                    "person_id": detection.get('person_id'),
                    "person_name": detection.get('person_name', 'Unknown'),
                    "confidence": detection.get('confidence', 0),
                    "timestamp": time.time(),
                    "bbox": detection.get('bbox')
                }
            }
            
            # Send to all connected clients (you might want to filter by user)
            await websocket_manager.broadcast_message(alert_message)
            print(f"âœ… Detection alert sent via WebSocket: {detection.get('person_name', 'Unknown')}")
            
        except Exception as e:
            print(f"Error sending detection alert: {e}")

    async def _get_known_persons_for_camera(self, camera_id: str) -> List[dict]:
        """Get known persons data for face recognition"""
        try:
            # Import here to avoid circular imports
            from ..database import get_database
            from bson import ObjectId
            
            db = get_database()
            known_persons = []
            
            print(f"ðŸ”µ Loading known persons for camera {camera_id}...")
            
            # Get all known persons (you might want to filter by camera or user)
            async for person_data in db.known_persons.find({"is_active": True}):
                # Convert embeddings - backend lÆ°u trong field 'face_embeddings'
                embeddings = []
                if 'face_embeddings' in person_data and person_data['face_embeddings']:
                    for emb_data in person_data['face_embeddings']:
                        if isinstance(emb_data, str) and emb_data:
                            # If stored as base64, decode it
                            try:
                                emb_bytes = base64.b64decode(emb_data)
                                emb_array = np.frombuffer(emb_bytes, dtype=np.float32)
                                embeddings.append(emb_array)
                            except Exception as e:
                                print(f"Error decoding base64 embedding: {e}")
                        elif isinstance(emb_data, list) and emb_data:
                            # If stored as list, convert to numpy array
                            try:
                                embeddings.append(np.array(emb_data, dtype=np.float32))
                            except Exception as e:
                                print(f"Error converting list to array: {e}")
                
                if embeddings:
                    # Äáº£m báº£o name Ä‘Æ°á»£c encode/decode Ä‘Ãºng UTF-8
                    person_name = person_data['name']
                    if isinstance(person_name, bytes):
                        person_name = person_name.decode('utf-8')
                    elif not isinstance(person_name, str):
                        person_name = str(person_name)
                    
                    known_persons.append({
                        'id': str(person_data['_id']),
                        'name': person_name,
                        'embeddings': embeddings
                    })
                    print(f"âœ… Loaded person: {person_name} with {len(embeddings)} embeddings")
                else:
                    person_name = person_data['name']
                    if isinstance(person_name, bytes):
                        person_name = person_name.decode('utf-8')
                    print(f"âš ï¸ Person {person_name} has no valid embeddings")
            
            print(f"âœ… Loaded {len(known_persons)} known persons for recognition")
            return known_persons
            
        except Exception as e:
            print(f"âŒ Error loading known persons: {e}")
            import traceback
            traceback.print_exc()
            return []

    async def _save_detection_to_database(self, camera_id: str, camera_name: str, detection: Dict[str, Any], frame: np.ndarray):
        """Save detection to database"""
        try:
            # Import here to avoid circular imports
            from ..database import get_database
            from bson import ObjectId
            from ..models.detection_log import DetectionLogCreate
            import base64
            import uuid
            import os
            from datetime import datetime
            
            # Get camera info
            db = get_database()
            camera_data = await db.cameras.find_one({"_id": ObjectId(camera_id)})
            if not camera_data:
                print(f"âŒ Camera not found: {camera_id}")
                return None
                
            user_id = str(camera_data.get("user_id", ""))
            if not user_id:
                print(f"âŒ No user_id found for camera: {camera_id}")
                return None
            
            # Convert frame to base64 for storage
            _, img_encoded = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
            img_base64 = base64.b64encode(img_encoded.tobytes()).decode('utf-8')
            
            # Create detection document
            detection_type = "known_person" if detection.get("person_name") != "Unknown" else "stranger"
            
            # Ensure uploads directory exists
            upload_dir = "uploads/detections"
            os.makedirs(upload_dir, exist_ok=True)
            
            # Save image to file
            image_filename = f"detection_{uuid.uuid4()}.jpg"
            image_path = os.path.join(upload_dir, image_filename)
            with open(image_path, 'wb') as f:
                f.write(img_encoded.tobytes())
                
            # Create database entry
            detection_doc = {
                "user_id": ObjectId(user_id),
                "camera_id": ObjectId(camera_id),
                "detection_type": detection_type,
                "person_id": ObjectId(detection.get("person_id")) if detection.get("person_id") else None,
                "person_name": detection.get("person_name", "Unknown"),
                "confidence": float(detection.get("confidence", 0)),
                "similarity_score": float(detection.get("recognition_confidence", 0)),
                "image_path": image_path,
                "bbox": detection.get("bbox", [0, 0, 0, 0]),
                "timestamp": datetime.utcnow(),
                "is_alert_sent": True,
                "alert_methods": ["websocket"],
                "metadata": {}
            }
            
            # Insert to database
            result = await db.detection_logs.insert_one(detection_doc)
            detection_id = str(result.inserted_id)
            
            print(f"âœ… Saved detection to database: {detection_id}, type: {detection_type}")
            return detection_id
            
        except Exception as e:
            import traceback
            print(f"âŒ Error saving detection to database: {e}")
            traceback.print_exc()
            return None

    async def _save_optimized_detection(self, camera_id: str, camera_name: str, detection: Dict[str, Any], frame: np.ndarray):
        """LÆ°u detection sá»­ dá»¥ng Detection Optimizer Service"""
        try:
            # Import here to avoid circular imports
            from ..database import get_database
            from bson import ObjectId
            import base64
            import os
            import uuid
            from datetime import datetime
            
            # Get camera info
            db = get_database()
            camera_data = await db.cameras.find_one({"_id": ObjectId(camera_id)})
            if not camera_data:
                print(f"âŒ Camera not found: {camera_id}")
                return None
                
            user_id = str(camera_data.get("user_id", ""))
            if not user_id:
                print(f"âŒ No user_id found for camera: {camera_id}")
                return None
            
            # Convert frame to base64 for storage
            _, img_encoded = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 90])
            img_base64 = base64.b64encode(img_encoded.tobytes()).decode('utf-8')
            
            # Create detection document
            detection_type = "known_person" if detection.get("person_name") != "Unknown" else "stranger"
            
            # Ensure uploads directory exists
            upload_dir = "uploads/detections"
            os.makedirs(upload_dir, exist_ok=True)
            
            # Save image to file
            image_filename = f"detection_{uuid.uuid4()}.jpg"
            image_path = os.path.join(upload_dir, image_filename)
            with open(image_path, 'wb') as f:
                f.write(img_encoded.tobytes())
            
            # Prepare detection data for optimizer
            detection_data = {
                "user_id": user_id,
                "camera_id": camera_id,
                "detection_type": detection_type,
                "person_id": detection.get("person_id"),
                "person_name": detection.get("person_name", "Unknown"),
                "confidence": float(detection.get("confidence", 0)),
                "similarity_score": float(detection.get("recognition_confidence", 0)),
                "image_path": image_path,
                "bbox": detection.get("bbox", [0, 0, 0, 0]),
                "timestamp": datetime.utcnow(),
                "is_alert_sent": True,
                "alert_methods": ["websocket"],
                "notes": f"Detected by {camera_name}"
            }
            
            try:
                # Use the existing detection optimizer from router
                from ..routers.detection_optimizer import detection_optimizer
                
                if detection_optimizer is not None:
                    # Process detection with optimizer
                    detection_id = await detection_optimizer.process_detection(detection_data)
                else:
                    # Fall back to traditional save if optimizer isn't available
                    print("âš ï¸ Detection optimizer service not available, falling back to traditional save")
                    detection_id = None  # Will be saved by the original method
            except Exception as optimizer_error:
                print(f"âš ï¸ Error using detection optimizer: {optimizer_error}")
                detection_id = None  # Will be saved by the original method
            
            if detection_id:
                print(f"âœ… Optimized detection saved: {detection.get('person_name')} - ID: {detection_id}")
            
            return detection_id
            
        except Exception as e:
            print(f"âŒ Error saving optimized detection: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _draw_utf8_text(self, frame: np.ndarray, text: str, position: tuple, 
                       font_scale: float = 0.8, color: tuple = (0, 255, 0), thickness: int = 2) -> np.ndarray:
        """Draw UTF-8 text (including Vietnamese) on frame using PIL"""
        try:
            # Convert OpenCV frame (BGR) to PIL Image (RGB)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            draw = ImageDraw.Draw(pil_image)
            
            # Try to load a font that supports Vietnamese characters
            font_size = int(20 * font_scale)
            try:
                # Try to use a system font that supports Vietnamese
                if os.name == 'nt':  # Windows
                    font_path = "C:/Windows/Fonts/arial.ttf"
                else:  # Linux/Mac
                    font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
                
                if os.path.exists(font_path):
                    font = ImageFont.truetype(font_path, font_size)
                else:
                    font = ImageFont.load_default()
            except:
                font = ImageFont.load_default()
            
            # Convert BGR color to RGB for PIL
            pil_color = (color[2], color[1], color[0])
            
            # Draw text with PIL
            draw.text(position, text, font=font, fill=pil_color)
            
            # Convert back to OpenCV format (RGB to BGR)
            frame_with_text = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
            return frame_with_text
            
        except Exception as e:
            print(f"Error drawing UTF-8 text: {e}")
            # Fallback to ASCII-only version if UTF-8 fails
            ascii_text = text.encode('ascii', 'ignore').decode('ascii')
            cv2.putText(frame, ascii_text, position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)
            return frame

# Global instance
stream_processor = StreamProcessor()